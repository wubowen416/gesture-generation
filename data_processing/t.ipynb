{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('/home/wu/repo/gesture-generation-using-WGAN/data/takekuchi/source/text/text21.json', 'r', 'utf8') as f:\n",
    "    words = json.load(f)['word_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_vec(words):\n",
    "    \"\"\"\n",
    "    Process recongized words as dict to bert embeddings\n",
    "    Args:\n",
    "        words - list of dict {'word', 'start_time', 'end_time'}\n",
    "    Returns:\n",
    "        features - embedded result mathcing the length of word_list\n",
    "    \"\"\"\n",
    "    num_words = [] # num of words in each recognized chunk\n",
    "    tokens = {\n",
    "        'input_ids': torch.LongTensor([[2]]), # 2 - 'sos'\n",
    "        'token_type_ids': torch.LongTensor([[0]]),\n",
    "        'attention_mask': torch.LongTensor([[1]])\n",
    "    }\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "\n",
    "        bert_tokens = tokenizer(word['word'], return_tensors='pt')\n",
    "\n",
    "        tokens['input_ids'] = torch.cat([tokens['input_ids'], bert_tokens['input_ids'][:, 1:-1]], dim=-1) \n",
    "        tokens['token_type_ids'] = torch.cat([tokens['token_type_ids'], bert_tokens['token_type_ids'][:, 1:-1]], dim=-1) \n",
    "        tokens['attention_mask'] = torch.cat([tokens['attention_mask'], bert_tokens['attention_mask'][:, 1:-1]], dim=-1) \n",
    "\n",
    "        num_words.append(bert_tokens['input_ids'].size(-1) - 2)\n",
    "\n",
    "    tokens['input_ids'] = torch.cat([tokens['input_ids'], torch.LongTensor([[3]])], dim=-1) # 3 - 'eof'\n",
    "    tokens['token_type_ids'] = torch.cat([tokens['token_type_ids'], torch.LongTensor([[0]])], dim=-1)\n",
    "    tokens['attention_mask'] = torch.cat([tokens['attention_mask'], torch.LongTensor([[1]])], dim=-1)\n",
    "\n",
    "    embeddings = model(**tokens)['last_hidden_state'].detach().numpy()[0] # features for each tokenized word\n",
    "\n",
    "    features = [] # features for each recongized chunk, may contain several words\n",
    "    start_word_index = 0\n",
    "    for i, num_word in enumerate(num_words):\n",
    "        end_word_index = start_word_index + num_word\n",
    "        if i == 0:\n",
    "            end_word_index += 1 # include 'sos'\n",
    "        if i == len(num_words) - 1:\n",
    "            end_word_index += 1  # include 'eos\n",
    "        # Average features for mulitple tokens in one word\n",
    "        feature = embeddings[start_word_index:end_word_index]\n",
    "        feature = np.mean(feature, axis=0)\n",
    "        # print(tokens['input_ids'][0][start_word_index:end_word_index])\n",
    "        features.append(feature)\n",
    "        start_word_index = end_word_index\n",
    "    features = np.stack(features)\n",
    "\n",
    "    assert len(features) == len(words), \"lenght of features must match length of words\"\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37c1ed16a00a63b5af7363df11059dd7eca776121b04b329e67abca37171ff84"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('wu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
